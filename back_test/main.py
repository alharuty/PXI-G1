from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import uvicorn
import os
from dotenv import load_dotenv
import requests
import xml.etree.ElementTree as ET
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from openai import OpenAI
import time

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

app = FastAPI(title="Simple RAG System", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class SearchRequest(BaseModel):
    query: str
    max_results: int = 5

class RAGRequest(BaseModel):
    topic: str
    question: str
    max_results: int = 5

class RAGResponse(BaseModel):
    answer: str
    sources: List[Dict]
    confidence: float

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π...")

# Embedding –º–æ–¥–µ–ª—å
try:
    embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')
    print("‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ embedding –º–æ–¥–µ–ª–∏: {e}")
    embedding_model = None

# LLM –º–æ–¥–µ–ª—å
llm_provider = os.getenv("LLM_PROVIDER", "openai")
openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

if llm_provider == "openai":
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if openai_api_key:
        openai_client = OpenAI(api_key=openai_api_key)
        print(f"‚úÖ OpenAI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {openai_model}")
    else:
        print("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")
        openai_client = None
else:
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if gemini_api_key:
        genai.configure(api_key=gemini_api_key)
        gemini_model = genai.GenerativeModel('gemini-1.5-pro')
        print("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    else:
        print("‚ùå GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")
        gemini_model = None

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
documents = []
document_embeddings = None

def search_arxiv(query: str, max_results: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ arXiv"""
    try:
        url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": f"all:{query}",
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance",
            "sortOrder": "descending"
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        
        root = ET.fromstring(response.content)
        articles = []
        
        for entry in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
            try:
                title = entry.find(".//{http://www.w3.org/2005/Atom}title")
                title_text = title.text.strip() if title is not None and title.text else "No title"
                
                summary = entry.find(".//{http://www.w3.org/2005/Atom}summary")
                summary_text = summary.text.strip() if summary is not None and summary.text else "No summary"
                
                link = entry.find(".//{http://www.w3.org/2005/Atom}id")
                link_text = link.text if link is not None else ""
                
                authors = []
                for author in entry.findall(".//{http://www.w3.org/2005/Atom}author"):
                    name = author.find(".//{http://www.w3.org/2005/Atom}name")
                    if name is not None and name.text:
                        authors.append(name.text.strip())
                
                article = {
                    "title": title_text,
                    "summary": summary_text,
                    "link": link_text,
                    "authors": authors,
                    "content": f"Title: {title_text}\n\nSummary: {summary_text}"
                }
                articles.append(article)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                continue
                
        return articles
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ arXiv: {e}")
        return []

def add_documents_to_rag(new_documents: List[Dict]):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
    global documents, document_embeddings
    
    documents.extend(new_documents)
    
    if embedding_model and documents:
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        texts = [doc["content"] for doc in documents]
        document_embeddings = embedding_model.encode(texts)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í—Å–µ–≥–æ: {len(documents)}")

def search_documents(query: str, top_k: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not embedding_model or document_embeddings is None or not documents:
        return []
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = embedding_model.encode([query])
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    similarities = cosine_similarity(query_embedding, document_embeddings)[0]
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.append({
                "document": documents[idx],
                "similarity": float(similarities[idx])
            })
    
    return results

def generate_answer(question: str, context_docs: List[Dict]) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM"""
    if not context_docs:
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = "\n\n".join([doc["document"]["content"] for doc in context_docs])
    
    # –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
    prompt = f"""–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏—Ç–µ –æ–± —ç—Ç–æ–º.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    try:
        if llm_provider == "openai" and openai_client:
            response = openai_client.chat.completions.create(
                model=openai_model,
                messages=[
                    {"role": "system", "content": "–í—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
        
        elif llm_provider == "gemini" and gemini_model:
            response = gemini_model.generate_content(prompt)
            return response.text
        
        else:
            return "–û—à–∏–±–∫–∞: LLM –º–æ–¥–µ–ª—å –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞"
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"

# API endpoints
@app.get("/")
async def root():
    return {"message": "Simple RAG System —Ä–∞–±–æ—Ç–∞–µ—Ç!", "docs_count": len(documents)}

@app.post("/search-arxiv")
async def search_arxiv_endpoint(request: SearchRequest):
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ arXiv"""
    articles = search_arxiv(request.query, request.max_results)
    return {"articles": articles, "count": len(articles)}

@app.post("/add-documents")
async def add_documents_endpoint(request: SearchRequest):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
    articles = search_arxiv(request.query, request.max_results)
    add_documents_to_rag(articles)
    return {"message": f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(articles)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "total_docs": len(documents)}

@app.post("/ask", response_model=RAGResponse)
async def ask_question_endpoint(request: RAGRequest):
    """–ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∫ RAG —Å–∏—Å—Ç–µ–º–µ"""
    if not documents:
        raise HTTPException(status_code=400, detail="–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ. –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
    
    # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    relevant_docs = search_documents(request.question, request.max_results)
    
    if not relevant_docs:
        return RAGResponse(
            answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
            sources=[],
            confidence=0.0
        )
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    answer = generate_answer(request.question, relevant_docs)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = []
    total_confidence = 0
    for doc in relevant_docs:
        sources.append({
            "title": doc["document"]["title"],
            "authors": doc["document"]["authors"],
            "link": doc["document"]["link"],
            "similarity": doc["similarity"]
        })
        total_confidence += doc["similarity"]
    
    avg_confidence = total_confidence / len(relevant_docs) if relevant_docs else 0
    
    return RAGResponse(
        answer=answer,
        sources=sources,
        confidence=avg_confidence
    )

@app.get("/stats")
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "total_documents": len(documents),
        "embedding_model": "BAAI/bge-large-en-v1.5" if embedding_model else "–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
        "llm_provider": llm_provider,
        "llm_model": openai_model if llm_provider == "openai" else "gemini-1.5-pro"
    }

@app.post("/clear")
async def clear_documents():
    """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    global documents, document_embeddings
    documents = []
    document_embeddings = None
    return {"message": "–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—á–∏—â–µ–Ω—ã"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000) 